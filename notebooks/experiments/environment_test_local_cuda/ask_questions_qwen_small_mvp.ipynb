{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import pprint\n",
    "import torch\n",
    "\n",
    "current_dir = Path(os.getcwd()).resolve()\n",
    "print(f\"Current dir: {current_dir.name}\")\n",
    "\n",
    "if current_dir.name == \"LLMPolReasonEval\": # uruchomione w Jupyter Lab\n",
    "    project_root = current_dir\n",
    "elif current_dir.name == \"content\": # uruchomione w Google Colab\n",
    "    project_root = current_dir / \"llm_pol_reason_eval\"\n",
    "else:  # uruchomione w PyCharm\n",
    "    project_root = current_dir.parents[2]\n",
    "print(f\"Project root: {project_root}\")\n",
    "src_dir = project_root / \"src\"\n",
    "print(f\"Checking if src directory exists: {src_dir.exists()}\")\n",
    "sys.path.append(str(src_dir))\n",
    "\n",
    "from llm_pol_reason_eval.qa_engine.llm_qa_engine import LLMQAEngine\n",
    "from llm_pol_reason_eval.qa_engine.inference_client import HuggingFaceClient"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "41bdec79c77a03a4",
   "metadata": {},
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA not enabled\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5809061874c1854",
   "metadata": {},
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Pamięć podręczna CUDA została wyczyszczona.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "29c65e6075de8453",
   "metadata": {},
   "source": [
    "EXPERIMENT_NAME = \"qwen-small-th-fs-cot-matura\" # lub \"bielik-small-precise-matura\"\n",
    "RUN_CONFIG_FILE = \"config/runs/qa_polski_matura_mvp_dataset.yaml\"\n",
    "MODELS_CONFIG_FILE = \"config/models.yaml\"\n",
    "\n",
    "with open(project_root / RUN_CONFIG_FILE, 'r', encoding='utf-8') as f:\n",
    "    run_config = yaml.safe_load(f)['experiments'][EXPERIMENT_NAME]\n",
    "\n",
    "with open(project_root / MODELS_CONFIG_FILE, 'r', encoding='utf-8') as f:\n",
    "    models_config = yaml.safe_load(f)\n",
    "\n",
    "print(f\"Uruchamiam eksperyment: {run_config.get('task_name')}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b971ec1938bb1381",
   "metadata": {},
   "source": [
    "model_key = run_config['model']\n",
    "model_cfg = models_config[model_key]\n",
    "\n",
    "run_overrides = run_config.get(\"param_overrides\", {})\n",
    "final_gen_params = model_cfg['generation_params'].copy()\n",
    "final_gen_params.update(run_overrides.get('default', {}))\n",
    "\n",
    "inference_client = HuggingFaceClient(\n",
    "    model_path=model_cfg['path'],\n",
    "    model_config=model_cfg,\n",
    "    default_generation_params=final_gen_params\n",
    ")\n",
    "\n",
    "engine = LLMQAEngine(\n",
    "    model_name=model_key,\n",
    "    model_path=model_cfg['path'],\n",
    "    inference_client=inference_client\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6f5164e1b3fcefae",
   "metadata": {},
   "source": [
    "input_dataset_path = project_root / run_config['input_dataset']\n",
    "output_dir = project_root / run_config['output_dir']\n",
    "iterations = run_config.get('iterations', 1)\n",
    "\n",
    "# Użyjemy query, żeby nie przetwarzać całego datasetu podczas testów w notatniku\n",
    "# target_question_ids = [\"MPOP-P1-100-A-2405_zadanie_14\", \"EPOP-P1-100-2305_zad_1\"]\n",
    "# query = lambda q: q.get(\"question_id\") in target_question_ids\n",
    "query = None\n",
    "\n",
    "for i in range(iterations):\n",
    "    run_output_dir = output_dir / f\"run_{i+1}\"\n",
    "    run_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_path = run_output_dir / f\"answers_{EXPERIMENT_NAME}.json\"\n",
    "\n",
    "    results = engine.generate_answers(\n",
    "        dataset_filepath=str(input_dataset_path),\n",
    "        output_filepath=str(output_path),\n",
    "        model_cfg={'name': model_key, 'family': model_cfg.get('family')},\n",
    "        prompt_composition=run_config.get(\"prompt_composition\", {}),\n",
    "        param_overrides=run_config.get(\"param_overrides\"),\n",
    "        query=query,\n",
    "        batch_size=10\n",
    "    )\n",
    "\n",
    "    print(\"--- WYGENEROWANE ODPOWIEDZI ---\")\n",
    "    pprint.pprint(results)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "346081ce32e22e24",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7f676f37987b1464",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
