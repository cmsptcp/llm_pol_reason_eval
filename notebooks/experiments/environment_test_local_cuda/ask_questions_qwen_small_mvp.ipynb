{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-06-16T01:39:15.364284Z",
     "start_time": "2025-06-16T01:39:04.202584Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import pprint\n",
    "import torch\n",
    "\n",
    "current_dir = Path(os.getcwd()).resolve()\n",
    "print(f\"Current dir: {current_dir.name}\")\n",
    "\n",
    "if current_dir.name == \"LLMPolReasonEval\": # uruchomione w Jupyter Lab\n",
    "    project_root = current_dir\n",
    "elif current_dir.name == \"content\": # uruchomione w Google Colab\n",
    "    project_root = current_dir / \"llm_pol_reason_eval\"\n",
    "else:  # uruchomione w PyCharm\n",
    "    project_root = current_dir.parents[2]\n",
    "print(f\"Project root: {project_root}\")\n",
    "src_dir = project_root / \"src\"\n",
    "print(f\"Checking if src directory exists: {src_dir.exists()}\")\n",
    "sys.path.append(str(src_dir))\n",
    "\n",
    "from llm_pol_reason_eval.qa_engine.llm_qa_engine import LLMQAEngine\n",
    "from llm_pol_reason_eval.qa_engine.inference_client import HuggingFaceClient"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dir: environment_test_local_cuda\n",
      "Project root: C:\\Users\\piotr\\PycharmProjects\\LLMPolReasonEval\n",
      "Checking if src directory exists: True\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "41bdec79c77a03a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T01:39:15.931945Z",
     "start_time": "2025-06-16T01:39:15.666662Z"
    }
   },
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA not enabled\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "5809061874c1854",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T01:39:16.209597Z",
     "start_time": "2025-06-16T01:39:16.023922Z"
    }
   },
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Pamięć podręczna CUDA została wyczyszczona.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pamięć podręczna CUDA została wyczyszczona.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "29c65e6075de8453",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T01:47:39.484823Z",
     "start_time": "2025-06-16T01:47:39.259462Z"
    }
   },
   "source": [
    "EXPERIMENT_NAME = \"qwen-small-th-fs-cot-matura\" # lub \"bielik-small-precise-matura\"\n",
    "RUN_CONFIG_FILE = \"config/runs/qa_polski_matura_mvp_dataset.yaml\"\n",
    "MODELS_CONFIG_FILE = \"config/models.yaml\"\n",
    "\n",
    "with open(project_root / RUN_CONFIG_FILE, 'r', encoding='utf-8') as f:\n",
    "    run_config = yaml.safe_load(f)['experiments'][EXPERIMENT_NAME]\n",
    "\n",
    "with open(project_root / MODELS_CONFIG_FILE, 'r', encoding='utf-8') as f:\n",
    "    models_config = yaml.safe_load(f)\n",
    "\n",
    "print(f\"Uruchamiam eksperyment: {run_config.get('task_name')}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uruchamiam eksperyment: Qwen3 1.7B - Z myśleniem, Few-Shot & CoT - Matura\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "b971ec1938bb1381",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T01:47:46.402259Z",
     "start_time": "2025-06-16T01:47:40.037683Z"
    }
   },
   "source": [
    "model_key = run_config['model']\n",
    "model_cfg = models_config[model_key]\n",
    "\n",
    "run_overrides = run_config.get(\"param_overrides\", {})\n",
    "final_gen_params = model_cfg['generation_params'].copy()\n",
    "final_gen_params.update(run_overrides.get('default', {}))\n",
    "\n",
    "inference_client = HuggingFaceClient(\n",
    "    model_path=model_cfg['path'],\n",
    "    model_config=model_cfg,\n",
    "    default_generation_params=final_gen_params\n",
    ")\n",
    "\n",
    "engine = LLMQAEngine(\n",
    "    model_name=model_key,\n",
    "    model_path=model_cfg['path'],\n",
    "    inference_client=inference_client\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFaceClient: Inicjalizacja modelu Qwen/Qwen3-1.7B na urządzeniu: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c95b6eedb4ab435f8da8a4cca850db26"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFaceClient: Domyślna konfiguracja generowania: GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"max_new_tokens\": 768,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "6f5164e1b3fcefae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T03:28:43.062267Z",
     "start_time": "2025-06-16T01:47:46.422368Z"
    }
   },
   "source": [
    "input_dataset_path = project_root / run_config['input_dataset']\n",
    "output_dir = project_root / run_config['output_dir']\n",
    "iterations = run_config.get('iterations', 1)\n",
    "\n",
    "# Użyjemy query, żeby nie przetwarzać całego datasetu podczas testów w notatniku\n",
    "# target_question_ids = [\"MPOP-P1-100-A-2405_zadanie_14\", \"EPOP-P1-100-2305_zad_1\"]\n",
    "# query = lambda q: q.get(\"question_id\") in target_question_ids\n",
    "query = None\n",
    "\n",
    "for i in range(iterations):\n",
    "    run_output_dir = output_dir / f\"run_{i+1}\"\n",
    "    run_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_path = run_output_dir / f\"answers_{EXPERIMENT_NAME}.json\"\n",
    "\n",
    "    results = engine.generate_answers(\n",
    "        dataset_filepath=str(input_dataset_path),\n",
    "        output_filepath=str(output_path),\n",
    "        model_cfg={'name': model_key, 'family': model_cfg.get('family')},\n",
    "        prompt_composition=run_config.get(\"prompt_composition\", {}),\n",
    "        param_overrides=run_config.get(\"param_overrides\"),\n",
    "        query=query,\n",
    "        batch_size=10\n",
    "    )\n",
    "\n",
    "    print(\"--- WYGENEROWANE ODPOWIEDZI ---\")\n",
    "    pprint.pprint(results)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ścieżka logów: C:\\Users\\piotr\\PycharmProjects\\LLMPolReasonEval\\results\\qwen-small-th-fs-cot-matura-mvp\\run_1\\logs\\mvp_dataset_2025-06-08T20-42-43Z_qwen3-1-7b_2025-06-16T01-47-46Z.log\n",
      "2025-06-16T01:47:46.623470+00:00 INFO: Logger uruchomiony.\n",
      "2025-06-16T01:47:46.624637+00:00 INFO: Ładowanie datasetu z: C:\\Users\\piotr\\PycharmProjects\\LLMPolReasonEval\\data\\dataset\\mvp_dataset_2025-06-08T20-42-43Z.json\n",
      "2025-06-16T01:47:46.628779+00:00 INFO: Załadowano 63 pytań.\n",
      "2025-06-16T01:47:46.628779+00:00 INFO: Tworzenie iteratora z rozmiarem batcha: 10.\n",
      "2025-06-16T01:47:46.629801+00:00 INFO: Przetwarzanie batcha 1 z 10 pytaniami.\n",
      "2025-06-16T01:47:46.631779+00:00 INFO: --- Batch 1: tryb wsadowy ---\n",
      "2025-06-16T01:47:46.670067+00:00 INFO: Wysyłanie 10 promptów do modelu z parametrami: {}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 15\u001B[39m\n\u001B[32m     12\u001B[39m run_output_dir.mkdir(parents=\u001B[38;5;28;01mTrue\u001B[39;00m, exist_ok=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m     13\u001B[39m output_path = run_output_dir / \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33manswers_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mEXPERIMENT_NAME\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.json\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m15\u001B[39m results = \u001B[43mengine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate_answers\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdataset_filepath\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43minput_dataset_path\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_filepath\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput_path\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     18\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_cfg\u001B[49m\u001B[43m=\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mname\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_key\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mfamily\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_cfg\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mfamily\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[43m    \u001B[49m\u001B[43mprompt_composition\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_config\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mprompt_composition\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     20\u001B[39m \u001B[43m    \u001B[49m\u001B[43mparam_overrides\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_config\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mparam_overrides\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     21\u001B[39m \u001B[43m    \u001B[49m\u001B[43mquery\u001B[49m\u001B[43m=\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     22\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m10\u001B[39;49m\n\u001B[32m     23\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     25\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m--- WYGENEROWANE ODPOWIEDZI ---\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     26\u001B[39m pprint.pprint(results)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LLMPolReasonEval\\src\\llm_pol_reason_eval\\qa_engine\\llm_qa_engine.py:50\u001B[39m, in \u001B[36mLLMQAEngine.generate_answers\u001B[39m\u001B[34m(self, dataset_filepath, output_filepath, model_cfg, prompt_composition, batch_size, query, param_overrides, skip_questions, max_questions)\u001B[39m\n\u001B[32m     48\u001B[39m \u001B[38;5;28mself\u001B[39m._load_dataset(dataset_filepath)\n\u001B[32m     49\u001B[39m batch_generator = \u001B[38;5;28mself\u001B[39m._create_question_batch_iterator(batch_size, query)\n\u001B[32m---> \u001B[39m\u001B[32m50\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_process_batches\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     51\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_generator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     52\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_cfg\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     53\u001B[39m \u001B[43m    \u001B[49m\u001B[43mprompt_composition\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     54\u001B[39m \u001B[43m    \u001B[49m\u001B[43mparam_overrides\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     55\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_filepath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     56\u001B[39m \u001B[43m    \u001B[49m\u001B[43mskip_questions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     57\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_questions\u001B[49m\n\u001B[32m     58\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     59\u001B[39m \u001B[38;5;28mself\u001B[39m.save_final_results(output_filepath)\n\u001B[32m     60\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.results\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LLMPolReasonEval\\src\\llm_pol_reason_eval\\qa_engine\\llm_qa_engine.py:91\u001B[39m, in \u001B[36mLLMQAEngine._process_batches\u001B[39m\u001B[34m(self, batch_generator, model_cfg, prompt_composition, param_overrides, output_filepath, skip_questions, max_questions)\u001B[39m\n\u001B[32m     89\u001B[39m \u001B[38;5;66;03m# Logika dyspozytorska: wybór trybu przetwarzania\u001B[39;00m\n\u001B[32m     90\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(questions_in_batch) > \u001B[32m1\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m91\u001B[39m     batch_results = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_handle_batched_inference\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     92\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbatch_data\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     93\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel_cfg\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel_cfg\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     94\u001B[39m \u001B[43m        \u001B[49m\u001B[43mprompt_composition\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprompt_composition\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     95\u001B[39m \u001B[43m        \u001B[49m\u001B[43mparam_overrides\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparam_overrides\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     96\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbatch_index\u001B[49m\u001B[43m=\u001B[49m\u001B[43mi\u001B[49m\n\u001B[32m     97\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     98\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     99\u001B[39m     batch_results = \u001B[38;5;28mself\u001B[39m._handle_serial_processing(\n\u001B[32m    100\u001B[39m         batch_data=batch_data,\n\u001B[32m    101\u001B[39m         model_cfg=model_cfg,\n\u001B[32m   (...)\u001B[39m\u001B[32m    104\u001B[39m         batch_index=i\n\u001B[32m    105\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LLMPolReasonEval\\src\\llm_pol_reason_eval\\qa_engine\\llm_qa_engine.py:152\u001B[39m, in \u001B[36mLLMQAEngine._handle_batched_inference\u001B[39m\u001B[34m(self, batch_data, model_cfg, prompt_composition, param_overrides, batch_index)\u001B[39m\n\u001B[32m    150\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, q_id \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(q_ids_in_order):\n\u001B[32m    151\u001B[39m     raw_response = raw_responses[i]\n\u001B[32m--> \u001B[39m\u001B[32m152\u001B[39m     parsed_answer = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_parse_single_answer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_response\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    153\u001B[39m     \u001B[38;5;28mself\u001B[39m.logger.info(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mSparsowana odpowiedź dla Q_ID \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mq_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparsed_answer[:\u001B[32m100\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    154\u001B[39m     batch_model_answers.append(ModelAnswerData(\n\u001B[32m    155\u001B[39m         model_answer_id=\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mans_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00muuid.uuid4()\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    156\u001B[39m         question_id=q_id,\n\u001B[32m   (...)\u001B[39m\u001B[32m    161\u001B[39m         model_configuration=model_config_details_json\n\u001B[32m    162\u001B[39m     ))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LLMPolReasonEval\\src\\llm_pol_reason_eval\\qa_engine\\llm_qa_engine.py:223\u001B[39m, in \u001B[36mLLMQAEngine._parse_single_answer\u001B[39m\u001B[34m(self, raw_answer_text)\u001B[39m\n\u001B[32m    222\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_parse_single_answer\u001B[39m(\u001B[38;5;28mself\u001B[39m, raw_answer_text: \u001B[38;5;28mstr\u001B[39m) -> \u001B[38;5;28mstr\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m223\u001B[39m     match = \u001B[43mre\u001B[49m.search(\u001B[33mr\u001B[39m\u001B[33m'\u001B[39m\u001B[33m<answer>(.*?)</answer>\u001B[39m\u001B[33m'\u001B[39m, raw_answer_text, re.DOTALL | re.IGNORECASE)\n\u001B[32m    224\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m match:\n\u001B[32m    225\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m match.group(\u001B[32m1\u001B[39m).strip()\n",
      "\u001B[31mNameError\u001B[39m: name 're' is not defined"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "346081ce32e22e24",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7f676f37987b1464",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
