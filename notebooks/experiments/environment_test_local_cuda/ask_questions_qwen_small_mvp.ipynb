{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-06-16T01:39:15.364284Z",
     "start_time": "2025-06-16T01:39:04.202584Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import pprint\n",
    "import torch\n",
    "\n",
    "current_dir = Path(os.getcwd()).resolve()\n",
    "print(f\"Current dir: {current_dir.name}\")\n",
    "\n",
    "if current_dir.name == \"LLMPolReasonEval\": # uruchomione w Jupyter Lab\n",
    "    project_root = current_dir\n",
    "elif current_dir.name == \"content\": # uruchomione w Google Colab\n",
    "    project_root = current_dir / \"llm_pol_reason_eval\"\n",
    "else:  # uruchomione w PyCharm\n",
    "    project_root = current_dir.parents[2]\n",
    "print(f\"Project root: {project_root}\")\n",
    "src_dir = project_root / \"src\"\n",
    "print(f\"Checking if src directory exists: {src_dir.exists()}\")\n",
    "sys.path.append(str(src_dir))\n",
    "\n",
    "from llm_pol_reason_eval.qa_engine.llm_qa_engine import LLMQAEngine\n",
    "from llm_pol_reason_eval.qa_engine.inference_client import HuggingFaceClient"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dir: environment_test_local_cuda\n",
      "Project root: C:\\Users\\piotr\\PycharmProjects\\LLMPolReasonEval\n",
      "Checking if src directory exists: True\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "41bdec79c77a03a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T01:39:15.931945Z",
     "start_time": "2025-06-16T01:39:15.666662Z"
    }
   },
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA not enabled\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "5809061874c1854",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T01:39:16.209597Z",
     "start_time": "2025-06-16T01:39:16.023922Z"
    }
   },
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Pamięć podręczna CUDA została wyczyszczona.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pamięć podręczna CUDA została wyczyszczona.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "29c65e6075de8453",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T01:47:39.484823Z",
     "start_time": "2025-06-16T01:47:39.259462Z"
    }
   },
   "source": [
    "EXPERIMENT_NAME = \"qwen-small-th-fs-cot-matura\" # lub \"bielik-small-precise-matura\"\n",
    "RUN_CONFIG_FILE = \"config/runs/qa_polski_matura_mvp_dataset.yaml\"\n",
    "MODELS_CONFIG_FILE = \"config/models.yaml\"\n",
    "\n",
    "with open(project_root / RUN_CONFIG_FILE, 'r', encoding='utf-8') as f:\n",
    "    run_config = yaml.safe_load(f)['experiments'][EXPERIMENT_NAME]\n",
    "\n",
    "with open(project_root / MODELS_CONFIG_FILE, 'r', encoding='utf-8') as f:\n",
    "    models_config = yaml.safe_load(f)\n",
    "\n",
    "print(f\"Uruchamiam eksperyment: {run_config.get('task_name')}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uruchamiam eksperyment: Qwen3 1.7B - Z myśleniem, Few-Shot & CoT - Matura\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "b971ec1938bb1381",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T01:47:46.402259Z",
     "start_time": "2025-06-16T01:47:40.037683Z"
    }
   },
   "source": [
    "model_key = run_config['model']\n",
    "model_cfg = models_config[model_key]\n",
    "\n",
    "run_overrides = run_config.get(\"param_overrides\", {})\n",
    "final_gen_params = model_cfg['generation_params'].copy()\n",
    "final_gen_params.update(run_overrides.get('default', {}))\n",
    "\n",
    "inference_client = HuggingFaceClient(\n",
    "    model_path=model_cfg['path'],\n",
    "    model_config=model_cfg,\n",
    "    default_generation_params=final_gen_params\n",
    ")\n",
    "\n",
    "engine = LLMQAEngine(\n",
    "    model_name=model_key,\n",
    "    model_path=model_cfg['path'],\n",
    "    inference_client=inference_client\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFaceClient: Inicjalizacja modelu Qwen/Qwen3-1.7B na urządzeniu: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c95b6eedb4ab435f8da8a4cca850db26"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFaceClient: Domyślna konfiguracja generowania: GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"max_new_tokens\": 768,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "6f5164e1b3fcefae",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-06-16T01:47:46.422368Z"
    }
   },
   "source": [
    "input_dataset_path = project_root / run_config['input_dataset']\n",
    "output_dir = project_root / run_config['output_dir']\n",
    "iterations = run_config.get('iterations', 1)\n",
    "\n",
    "# Użyjemy query, żeby nie przetwarzać całego datasetu podczas testów w notatniku\n",
    "# target_question_ids = [\"MPOP-P1-100-A-2405_zadanie_14\", \"EPOP-P1-100-2305_zad_1\"]\n",
    "# query = lambda q: q.get(\"question_id\") in target_question_ids\n",
    "query = None\n",
    "\n",
    "for i in range(iterations):\n",
    "    run_output_dir = output_dir / f\"run_{i+1}\"\n",
    "    run_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_path = run_output_dir / f\"answers_{EXPERIMENT_NAME}.json\"\n",
    "\n",
    "    results = engine.generate_answers(\n",
    "        dataset_filepath=str(input_dataset_path),\n",
    "        output_filepath=str(output_path),\n",
    "        model_cfg={'name': model_key, 'family': model_cfg.get('family')},\n",
    "        prompt_composition=run_config.get(\"prompt_composition\", {}),\n",
    "        param_overrides=run_config.get(\"param_overrides\"),\n",
    "        query=query,\n",
    "        batch_size=10\n",
    "    )\n",
    "\n",
    "    print(\"--- WYGENEROWANE ODPOWIEDZI ---\")\n",
    "    pprint.pprint(results)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ścieżka logów: C:\\Users\\piotr\\PycharmProjects\\LLMPolReasonEval\\results\\qwen-small-th-fs-cot-matura-mvp\\run_1\\logs\\mvp_dataset_2025-06-08T20-42-43Z_qwen3-1-7b_2025-06-16T01-47-46Z.log\n",
      "2025-06-16T01:47:46.623470+00:00 INFO: Logger uruchomiony.\n",
      "2025-06-16T01:47:46.624637+00:00 INFO: Ładowanie datasetu z: C:\\Users\\piotr\\PycharmProjects\\LLMPolReasonEval\\data\\dataset\\mvp_dataset_2025-06-08T20-42-43Z.json\n",
      "2025-06-16T01:47:46.628779+00:00 INFO: Załadowano 63 pytań.\n",
      "2025-06-16T01:47:46.628779+00:00 INFO: Tworzenie iteratora z rozmiarem batcha: 10.\n",
      "2025-06-16T01:47:46.629801+00:00 INFO: Przetwarzanie batcha 1 z 10 pytaniami.\n",
      "2025-06-16T01:47:46.631779+00:00 INFO: --- Batch 1: tryb wsadowy ---\n",
      "2025-06-16T01:47:46.670067+00:00 INFO: Wysyłanie 10 promptów do modelu z parametrami: {}\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "346081ce32e22e24",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7f676f37987b1464",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
