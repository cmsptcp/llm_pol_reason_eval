{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-15T23:53:14.777461Z",
     "start_time": "2025-06-15T23:53:14.485315Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import pprint\n",
    "import torch\n",
    "\n",
    "current_dir = Path(os.getcwd()).resolve()\n",
    "print(f\"Current dir: {current_dir.name}\")\n",
    "\n",
    "if current_dir.name == \"LLMPolReasonEval\": # uruchomione w Jupyter Lab\n",
    "    project_root = current_dir\n",
    "elif current_dir.name == \"content\": # uruchomione w Google Colab\n",
    "    project_root = current_dir / \"llm_pol_reason_eval\"\n",
    "else:  # uruchomione w PyCharm\n",
    "    project_root = current_dir.parents[2]\n",
    "print(f\"Project root: {project_root}\")\n",
    "src_dir = project_root / \"src\"\n",
    "if src_dir.exists() and str(src_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(src_dir))\n",
    "    print(f\"Directory {src_dir} added to sys.path\")\n",
    "\n",
    "from llm_pol_reason_eval.qa_engine.llm_qa_engine import LLMQAEngine\n",
    "from llm_pol_reason_eval.qa_engine.inference_client import HuggingFaceClient"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Current dir: environment_test_local_cuda\n",
      "Project root: C:\\Users\\piotr\\PycharmProjects\\LLMPolReasonEval\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:53:15.763477Z",
     "start_time": "2025-06-15T23:53:15.558125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA not enabled\")"
   ],
   "id": "23aa58246594ae24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:53:16.878061Z",
     "start_time": "2025-06-15T23:53:16.415293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Pamięć podręczna CUDA została wyczyszczona.\")"
   ],
   "id": "d0a8cc24ccf202ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pamięć podręczna CUDA została wyczyszczona.\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:58:00.490137Z",
     "start_time": "2025-06-15T23:58:00.116192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# EXPERIMENT_NAME = \"bielik-small-matura\" # lub \"qwen-small-th-fs-cot-matura\"\n",
    "EXPERIMENT_NAME = \"bielik-small-fs-cot-matura\"\n",
    "RUN_CONFIG_FILE = \"config/runs/qa_polski_matura_dataset.yaml\"\n",
    "MODELS_CONFIG_FILE = \"config/models.yaml\"\n",
    "\n",
    "with open(project_root / RUN_CONFIG_FILE, 'r', encoding='utf-8') as f:\n",
    "    run_config = yaml.safe_load(f)['experiments'][EXPERIMENT_NAME]\n",
    "\n",
    "with open(project_root / MODELS_CONFIG_FILE, 'r', encoding='utf-8') as f:\n",
    "    models_config = yaml.safe_load(f)\n",
    "\n",
    "print(f\"Uruchamiam eksperyment: {run_config.get('task_name')}\")"
   ],
   "id": "49c22bc90308993d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uruchamiam eksperyment: Bielik 1.5B - Few-Shot & CoT - Matura\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:58:26.251459Z",
     "start_time": "2025-06-15T23:58:23.018474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_key = run_config['model']\n",
    "model_cfg = models_config[model_key]\n",
    "\n",
    "run_overrides = run_config.get(\"param_overrides\", {})\n",
    "final_gen_params = model_cfg['generation_params'].copy()\n",
    "final_gen_params.update(run_overrides.get('default', {}))\n",
    "\n",
    "inference_client = HuggingFaceClient(\n",
    "    model_path=model_cfg['path'],\n",
    "    default_generation_params=final_gen_params\n",
    ")\n",
    "\n",
    "qa_engine = LLMQAEngine(\n",
    "    model_name=model_key,\n",
    "    model_path=model_cfg['path'],\n",
    "    inference_client=inference_client\n",
    ")"
   ],
   "id": "258e250d144a3e07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFaceClient: Inicjalizacja modelu speakleash/Bielik-1.5B-v3.0-Instruct na urzÄ…dzeniu: cuda\n",
      "HuggingFaceClient: DomyĹ›lna konfiguracja generowania: GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    4,\n",
      "    2\n",
      "  ],\n",
      "  \"max_new_tokens\": 512,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"temperature\": 0.3\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:58:26.505036Z",
     "start_time": "2025-06-15T23:58:26.287336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_dataset_path = project_root / run_config['input_dataset']\n",
    "output_dir = project_root / run_config['output_dir']\n",
    "iterations = run_config.get('iterations', 1)\n",
    "query=None"
   ],
   "id": "82131dd1f25656d6",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:58:29.000573Z",
     "start_time": "2025-06-15T23:58:28.801411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "run_id = 1\n",
    "run_output_dir = output_dir / f\"run_{run_id}\"\n",
    "run_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "answers_output_path = run_output_dir / f\"answers_{EXPERIMENT_NAME}.json\""
   ],
   "id": "e7aa421a8d603a86",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:58:34.366982Z",
     "start_time": "2025-06-15T23:58:34.160724Z"
    }
   },
   "cell_type": "code",
   "source": "qa_engine._setup_logger_and_reset_results(input_dataset_path, answers_output_path)",
   "id": "beba68ed600dcd5b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ścieżka logów: C:\\Users\\piotr\\PycharmProjects\\LLMPolReasonEval\\results\\bielik-small-fs-cot-matura\\run_1\\logs\\polski_matura_dataset_2025-06-14T20-00-06Z_bielik-1-5b-v3-instruct_2025-06-15T23-58-34Z.log\n",
      "2025-06-15T23:58:34.364008+00:00 INFO: Logger uruchomiony.\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:58:36.146328Z",
     "start_time": "2025-06-15T23:58:35.933148Z"
    }
   },
   "cell_type": "code",
   "source": "qa_engine._load_dataset(input_dataset_path)",
   "id": "d815c7e04b9d2f61",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-15T23:58:36.131538+00:00 INFO: Ładowanie datasetu z: C:\\Users\\piotr\\PycharmProjects\\LLMPolReasonEval\\data\\dataset\\polski_matura_dataset_2025-06-14T20-00-06Z.json\n",
      "2025-06-15T23:58:36.143042+00:00 INFO: Załadowano 267 pytań i 104 kontekstów.\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:58:37.679076Z",
     "start_time": "2025-06-15T23:58:37.472809Z"
    }
   },
   "cell_type": "code",
   "source": "qa_engine.dataset_manager.get_stats()",
   "id": "dfb6c8519d4734ab",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_category_stats': [('matura_język_polski', 267)],\n",
       " 'question_type_stats': [('open_text', 179),\n",
       "  ('closed_MTF', 28),\n",
       "  ('closed_MCQ', 16),\n",
       "  ('open_essay', 13),\n",
       "  ('open_summary', 10),\n",
       "  ('open_poetry_interpretation', 8),\n",
       "  ('closed_MRQ', 7),\n",
       "  ('open_synthesis', 6)]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:58:40.153233Z",
     "start_time": "2025-06-15T23:58:39.948996Z"
    }
   },
   "cell_type": "code",
   "source": "prompt_composition = run_config.get(\"prompt_composition\", {})",
   "id": "9489525dda49e559",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:58:42.202598Z",
     "start_time": "2025-06-15T23:58:41.987640Z"
    }
   },
   "cell_type": "code",
   "source": "batch_generator = qa_engine._create_question_batch_iterator(1,query=None)",
   "id": "87f0324aa641e83d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-15T23:58:42.195448+00:00 INFO: Tworzenie iteratora batchy pytań z rozmiarem batcha: 1.\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T00:01:20.979940Z",
     "start_time": "2025-06-15T23:58:49.529063Z"
    }
   },
   "cell_type": "code",
   "source": "qa_engine._process_batches(batch_generator, model_cfg, prompt_composition, {}, answers_output_path, max_questions=5)",
   "id": "c95ea760083ac7c2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-15T23:58:49.729519+00:00 INFO: Przetwarzanie batcha 1 zawierającego 1 pytań.\n",
      "2025-06-15T23:58:49.729519+00:00 INFO: --- Rozpoczęcie batcha 1 | Kategoria: matura_język_polski, Typ (z metadanych): closed_MTF, Liczba pytań: 1 ---\n",
      "2025-06-15T23:58:49.730656+00:00 INFO: Przetwarzanie pytania (ID: EPOP-P1-100-2105_zad1) z batcha 1.\n",
      "2025-06-15T23:58:49.744707+00:00 INFO: Finalny prompt dla Q_ID EPOP-P1-100-2105_zad1 (pierwsze 200 znaków): <s><|im_start|>system\n",
      "Twoja rola to wzorowy uczeń liceum, który rozwiązuje zadania z arkusza maturalnego z języka polskiego. Analizuj polecenia i załączone teksty z najwyższą starannością.\n",
      "\n",
      "Twoje odpo...\n",
      "2025-06-15T23:58:49.744707+00:00 INFO: ... (całkowita długość promptu: 15580 znaków)\n",
      "2025-06-15T23:58:49.745711+00:00 INFO: Wysyłanie promptu do modelu z parametrami: {}\n",
      "2025-06-15T23:59:20.516571+00:00 INFO: Otrzymano surową odpowiedź od modelu (pierwsze 200 znaków): <answer>\n",
      "1. P\n",
      "2. F\n",
      "3. P\n",
      "</answer>...\n",
      "2025-06-15T23:59:20.517572+00:00 INFO: Sparsowana odpowiedź dla Q_ID EPOP-P1-100-2105_zad1: 1. P\n",
      "2. F\n",
      "3. P\n",
      "2025-06-15T23:59:20.519528+00:00 INFO: Zapisano częściowe wyniki po batchu 1. Przetworzono 1 pytań w tym batchu. Łącznie przetworzonych pytań: 1. Łącznie zapisanych odpowiedzi: 1.\n",
      "2025-06-15T23:59:20.519528+00:00 INFO: Przetwarzanie batcha 2 zawierającego 1 pytań.\n",
      "2025-06-15T23:59:20.519528+00:00 INFO: --- Rozpoczęcie batcha 2 | Kategoria: matura_język_polski, Typ (z metadanych): closed_MTF, Liczba pytań: 1 ---\n",
      "2025-06-15T23:59:20.520530+00:00 INFO: Przetwarzanie pytania (ID: EPOP-P1-100-2105_zad8) z batcha 2.\n",
      "2025-06-15T23:59:20.521526+00:00 INFO: Finalny prompt dla Q_ID EPOP-P1-100-2105_zad8 (pierwsze 200 znaków): <s><|im_start|>system\n",
      "Twoja rola to wzorowy uczeń liceum, który rozwiązuje zadania z arkusza maturalnego z języka polskiego. Analizuj polecenia i załączone teksty z najwyższą starannością.\n",
      "\n",
      "Twoje odpo...\n",
      "2025-06-15T23:59:20.522572+00:00 INFO: ... (całkowita długość promptu: 15053 znaków)\n",
      "2025-06-15T23:59:20.522572+00:00 INFO: Wysyłanie promptu do modelu z parametrami: {}\n",
      "2025-06-15T23:59:50.630343+00:00 INFO: Otrzymano surową odpowiedź od modelu (pierwsze 200 znaków): <answer>\n",
      "1. P\n",
      "2. F\n",
      "3. P\n",
      "</answer>...\n",
      "2025-06-15T23:59:50.631307+00:00 INFO: Sparsowana odpowiedź dla Q_ID EPOP-P1-100-2105_zad8: 1. P\n",
      "2. F\n",
      "3. P\n",
      "2025-06-15T23:59:50.633301+00:00 INFO: Zapisano częściowe wyniki po batchu 2. Przetworzono 1 pytań w tym batchu. Łącznie przetworzonych pytań: 2. Łącznie zapisanych odpowiedzi: 2.\n",
      "2025-06-15T23:59:50.634301+00:00 INFO: Przetwarzanie batcha 3 zawierającego 1 pytań.\n",
      "2025-06-15T23:59:50.634301+00:00 INFO: --- Rozpoczęcie batcha 3 | Kategoria: matura_język_polski, Typ (z metadanych): closed_MTF, Liczba pytań: 1 ---\n",
      "2025-06-15T23:59:50.635334+00:00 INFO: Przetwarzanie pytania (ID: EPOP-P1-100-2205_zadanie_1) z batcha 3.\n",
      "2025-06-15T23:59:50.637334+00:00 INFO: Finalny prompt dla Q_ID EPOP-P1-100-2205_zadanie_1 (pierwsze 200 znaków): <s><|im_start|>system\n",
      "Twoja rola to wzorowy uczeń liceum, który rozwiązuje zadania z arkusza maturalnego z języka polskiego. Analizuj polecenia i załączone teksty z najwyższą starannością.\n",
      "\n",
      "Twoje odpo...\n",
      "2025-06-15T23:59:50.638297+00:00 INFO: ... (całkowita długość promptu: 15018 znaków)\n",
      "2025-06-15T23:59:50.638297+00:00 INFO: Wysyłanie promptu do modelu z parametrami: {}\n",
      "2025-06-16T00:00:20.791631+00:00 INFO: Otrzymano surową odpowiedź od modelu (pierwsze 200 znaków): <answer>\n",
      "1. TAK\n",
      "2. TAK\n",
      "3. NIE\n",
      "</answer>...\n",
      "2025-06-16T00:00:20.791631+00:00 INFO: Sparsowana odpowiedź dla Q_ID EPOP-P1-100-2205_zadanie_1: 1. TAK\n",
      "2. TAK\n",
      "3. NIE\n",
      "2025-06-16T00:00:20.791631+00:00 INFO: Zapisano częściowe wyniki po batchu 3. Przetworzono 1 pytań w tym batchu. Łącznie przetworzonych pytań: 3. Łącznie zapisanych odpowiedzi: 3.\n",
      "2025-06-16T00:00:20.791631+00:00 INFO: Przetwarzanie batcha 4 zawierającego 1 pytań.\n",
      "2025-06-16T00:00:20.791631+00:00 INFO: --- Rozpoczęcie batcha 4 | Kategoria: matura_język_polski, Typ (z metadanych): closed_MTF, Liczba pytań: 1 ---\n",
      "2025-06-16T00:00:20.791631+00:00 INFO: Przetwarzanie pytania (ID: EPOP-P1-100-2205_zadanie_12) z batcha 4.\n",
      "2025-06-16T00:00:20.791631+00:00 INFO: Finalny prompt dla Q_ID EPOP-P1-100-2205_zadanie_12 (pierwsze 200 znaków): <s><|im_start|>system\n",
      "Twoja rola to wzorowy uczeń liceum, który rozwiązuje zadania z arkusza maturalnego z języka polskiego. Analizuj polecenia i załączone teksty z najwyższą starannością.\n",
      "\n",
      "Twoje odpo...\n",
      "2025-06-16T00:00:20.791631+00:00 INFO: ... (całkowita długość promptu: 14262 znaków)\n",
      "2025-06-16T00:00:20.791631+00:00 INFO: Wysyłanie promptu do modelu z parametrami: {}\n",
      "2025-06-16T00:00:50.682890+00:00 INFO: Otrzymano surową odpowiedź od modelu (pierwsze 200 znaków): <answer>\n",
      "1. P\n",
      "2. P\n",
      "3. F\n",
      "</answer>...\n",
      "2025-06-16T00:00:50.682890+00:00 INFO: Sparsowana odpowiedź dla Q_ID EPOP-P1-100-2205_zadanie_12: 1. P\n",
      "2. P\n",
      "3. F\n",
      "2025-06-16T00:00:50.682890+00:00 INFO: Zapisano częściowe wyniki po batchu 4. Przetworzono 1 pytań w tym batchu. Łącznie przetworzonych pytań: 4. Łącznie zapisanych odpowiedzi: 4.\n",
      "2025-06-16T00:00:50.682890+00:00 INFO: Przetwarzanie batcha 5 zawierającego 1 pytań.\n",
      "2025-06-16T00:00:50.682890+00:00 INFO: --- Rozpoczęcie batcha 5 | Kategoria: matura_język_polski, Typ (z metadanych): closed_MTF, Liczba pytań: 1 ---\n",
      "2025-06-16T00:00:50.682890+00:00 INFO: Przetwarzanie pytania (ID: EPOP-P1-100-2305_zad_1) z batcha 5.\n",
      "2025-06-16T00:00:50.682890+00:00 INFO: Finalny prompt dla Q_ID EPOP-P1-100-2305_zad_1 (pierwsze 200 znaków): <s><|im_start|>system\n",
      "Twoja rola to wzorowy uczeń liceum, który rozwiązuje zadania z arkusza maturalnego z języka polskiego. Analizuj polecenia i załączone teksty z najwyższą starannością.\n",
      "\n",
      "Twoje odpo...\n",
      "2025-06-16T00:00:50.682890+00:00 INFO: ... (całkowita długość promptu: 14441 znaków)\n",
      "2025-06-16T00:00:50.682890+00:00 INFO: Wysyłanie promptu do modelu z parametrami: {}\n",
      "2025-06-16T00:01:20.958030+00:00 INFO: Otrzymano surową odpowiedź od modelu (pierwsze 200 znaków): <answer>\n",
      "1. P\n",
      "2. F\n",
      "3. P\n",
      "</answer>...\n",
      "2025-06-16T00:01:20.958030+00:00 INFO: Sparsowana odpowiedź dla Q_ID EPOP-P1-100-2305_zad_1: 1. P\n",
      "2. F\n",
      "3. P\n",
      "2025-06-16T00:01:20.973998+00:00 INFO: Zapisano częściowe wyniki po batchu 5. Przetworzono 1 pytań w tym batchu. Łącznie przetworzonych pytań: 5. Łącznie zapisanych odpowiedzi: 5.\n",
      "2025-06-16T00:01:20.976097+00:00 INFO: Osiągnięto limit 5 pytań. Przerywam przetwarzanie.\n",
      "2025-06-16T00:01:20.976097+00:00 INFO: Zakończono przetwarzanie wszystkich batchy. Łącznie przetworzono 5 pytań.\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T00:01:21.307628Z",
     "start_time": "2025-06-16T00:01:21.070103Z"
    }
   },
   "cell_type": "code",
   "source": "qa_engine.results",
   "id": "4c371476e049d76a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model_answer_id': 'ans_089fc16c-cf40-4a74-b9b8-629832c4dcc6',\n",
       "  'question_id': 'EPOP-P1-100-2105_zad1',\n",
       "  'model_answer_raw_text': '<answer>\\n1. P\\n2. F\\n3. P\\n</answer>',\n",
       "  'model_answer_clean_text': '1. P\\n2. F\\n3. P',\n",
       "  'generated_by': 'bielik-1-5b-v3-instruct (speakleash/Bielik-1.5B-v3.0-Instruct)',\n",
       "  'generation_date': '2025-06-15T23:59:20.518563+00:00',\n",
       "  'model_configuration': '{\"model_config\": {\"name\": \"bielik-1-5b-v3-instruct\", \"path\": \"speakleash/Bielik-1.5B-v3.0-Instruct\", \"family\": \"bielik\", \"generation_params\": {\"max_new_tokens\": 512, \"temperature\": 0.3, \"do_sample\": true}}, \"prompt_composition\": {\"main_template\": \"base_question_prompt.jinja2\", \"components\": [\"few_shot\", \"chain_of_thought\"], \"template_params\": {}}, \"generation_parameters\": {}, \"tokenizer_arguments\": {\"enable_thinking\": false}}'},\n",
       " {'model_answer_id': 'ans_ae3267e5-4bf9-4270-ac31-3459d013d69e',\n",
       "  'question_id': 'EPOP-P1-100-2105_zad8',\n",
       "  'model_answer_raw_text': '<answer>\\n1. P\\n2. F\\n3. P\\n</answer>',\n",
       "  'model_answer_clean_text': '1. P\\n2. F\\n3. P',\n",
       "  'generated_by': 'bielik-1-5b-v3-instruct (speakleash/Bielik-1.5B-v3.0-Instruct)',\n",
       "  'generation_date': '2025-06-15T23:59:50.632298+00:00',\n",
       "  'model_configuration': '{\"model_config\": {\"name\": \"bielik-1-5b-v3-instruct\", \"path\": \"speakleash/Bielik-1.5B-v3.0-Instruct\", \"family\": \"bielik\", \"generation_params\": {\"max_new_tokens\": 512, \"temperature\": 0.3, \"do_sample\": true}}, \"prompt_composition\": {\"main_template\": \"base_question_prompt.jinja2\", \"components\": [\"few_shot\", \"chain_of_thought\"], \"template_params\": {}}, \"generation_parameters\": {}, \"tokenizer_arguments\": {\"enable_thinking\": false}}'},\n",
       " {'model_answer_id': 'ans_b1043af2-d983-4463-9745-1168e672aba7',\n",
       "  'question_id': 'EPOP-P1-100-2205_zadanie_1',\n",
       "  'model_answer_raw_text': '<answer>\\n1. TAK\\n2. TAK\\n3. NIE\\n</answer>',\n",
       "  'model_answer_clean_text': '1. TAK\\n2. TAK\\n3. NIE',\n",
       "  'generated_by': 'bielik-1-5b-v3-instruct (speakleash/Bielik-1.5B-v3.0-Instruct)',\n",
       "  'generation_date': '2025-06-16T00:00:20.791631+00:00',\n",
       "  'model_configuration': '{\"model_config\": {\"name\": \"bielik-1-5b-v3-instruct\", \"path\": \"speakleash/Bielik-1.5B-v3.0-Instruct\", \"family\": \"bielik\", \"generation_params\": {\"max_new_tokens\": 512, \"temperature\": 0.3, \"do_sample\": true}}, \"prompt_composition\": {\"main_template\": \"base_question_prompt.jinja2\", \"components\": [\"few_shot\", \"chain_of_thought\"], \"template_params\": {}}, \"generation_parameters\": {}, \"tokenizer_arguments\": {\"enable_thinking\": false}}'},\n",
       " {'model_answer_id': 'ans_935b7944-97bd-491f-b416-2301de43ec4b',\n",
       "  'question_id': 'EPOP-P1-100-2205_zadanie_12',\n",
       "  'model_answer_raw_text': '<answer>\\n1. P\\n2. P\\n3. F\\n</answer>',\n",
       "  'model_answer_clean_text': '1. P\\n2. P\\n3. F',\n",
       "  'generated_by': 'bielik-1-5b-v3-instruct (speakleash/Bielik-1.5B-v3.0-Instruct)',\n",
       "  'generation_date': '2025-06-16T00:00:50.682890+00:00',\n",
       "  'model_configuration': '{\"model_config\": {\"name\": \"bielik-1-5b-v3-instruct\", \"path\": \"speakleash/Bielik-1.5B-v3.0-Instruct\", \"family\": \"bielik\", \"generation_params\": {\"max_new_tokens\": 512, \"temperature\": 0.3, \"do_sample\": true}}, \"prompt_composition\": {\"main_template\": \"base_question_prompt.jinja2\", \"components\": [\"few_shot\", \"chain_of_thought\"], \"template_params\": {}}, \"generation_parameters\": {}, \"tokenizer_arguments\": {\"enable_thinking\": false}}'},\n",
       " {'model_answer_id': 'ans_0d3c0ea7-fa59-470d-939d-ca6acf44cf2b',\n",
       "  'question_id': 'EPOP-P1-100-2305_zad_1',\n",
       "  'model_answer_raw_text': '<answer>\\n1. P\\n2. F\\n3. P\\n</answer>',\n",
       "  'model_answer_clean_text': '1. P\\n2. F\\n3. P',\n",
       "  'generated_by': 'bielik-1-5b-v3-instruct (speakleash/Bielik-1.5B-v3.0-Instruct)',\n",
       "  'generation_date': '2025-06-16T00:01:20.973998+00:00',\n",
       "  'model_configuration': '{\"model_config\": {\"name\": \"bielik-1-5b-v3-instruct\", \"path\": \"speakleash/Bielik-1.5B-v3.0-Instruct\", \"family\": \"bielik\", \"generation_params\": {\"max_new_tokens\": 512, \"temperature\": 0.3, \"do_sample\": true}}, \"prompt_composition\": {\"main_template\": \"base_question_prompt.jinja2\", \"components\": [\"few_shot\", \"chain_of_thought\"], \"template_params\": {}}, \"generation_parameters\": {}, \"tokenizer_arguments\": {\"enable_thinking\": false}}'}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "qa_engine._",
   "id": "efbd8fe8dcb784b8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
